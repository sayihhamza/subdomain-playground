{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shopify Subdomain Takeover Scanner - Dataset-Based\n",
    "\n",
    "**Simple workflow:**\n",
    "1. Load CSV dataset\n",
    "2. Filter by HTTP status (403, 404, 409, 500, 503)\n",
    "3. Sort by Est Monthly Page Views (highest traffic first)\n",
    "4. Export top N targets\n",
    "5. Deep scan with validation tools\n",
    "\n",
    "**Total time:** 15-20 minutes for 100 domains\n",
    "\n",
    "**Dataset:** `/kaggle/input/all-leads-merged/results.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Clone Project from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "WORKDIR=\"/kaggle/working\"\n",
    "PROJECT_DIR=\"$WORKDIR/subdomain-playground\"\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Cloning Project from GitHub\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "mkdir -p \"$WORKDIR\"\n",
    "cd \"$WORKDIR\"\n",
    "\n",
    "if [ -d \"$PROJECT_DIR\" ]; then\n",
    "    echo \"Removing existing copy...\"\n",
    "    rm -rf \"$PROJECT_DIR\"\n",
    "fi\n",
    "\n",
    "git clone --depth 1 https://github.com/sayihhamza/subdomain-playground.git subdomain-playground\n",
    "\n",
    "if [ ! -d \"$PROJECT_DIR\" ]; then\n",
    "    echo \"‚úó Clone failed!\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "cd \"$PROJECT_DIR\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Project cloned successfully!\"\n",
    "echo \"\"\n",
    "echo \"Key files:\"\n",
    "ls -lh scan.py hybrid_scan.py 2>/dev/null || echo \"Files available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working/subdomain-playground\n",
    "\n",
    "echo \"Installing Python requirements...\"\n",
    "python3 -m pip install --quiet -r requirements.txt\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Dependencies installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Go 1.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tmp\n",
    "\n",
    "if command -v sudo >/dev/null 2>&1; then\n",
    "    SUDO=\"sudo\"\n",
    "else\n",
    "    SUDO=\"\"\n",
    "fi\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Installing Go 1.24.1\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "$SUDO rm -rf /usr/local/go 2>/dev/null || true\n",
    "\n",
    "echo \"Downloading Go 1.24.1...\"\n",
    "wget -q https://go.dev/dl/go1.24.1.linux-amd64.tar.gz -O /tmp/go.tar.gz\n",
    "\n",
    "$SUDO tar -C /usr/local -xzf /tmp/go.tar.gz\n",
    "rm -f /tmp/go.tar.gz\n",
    "\n",
    "echo \"\"\n",
    "echo \"‚úì Go 1.24.1 installed!\"\n",
    "/usr/local/go/bin/go version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Build Security Tools\n",
    "\n",
    "**Building 5 tools (5-8 minutes):**\n",
    "- subfinder: Passive enumeration\n",
    "- findomain: Additional coverage (+5-10%)\n",
    "- dnsx: DNS + CNAME validation\n",
    "- httpx: HTTP validation\n",
    "- subzy: Takeover detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\nexport PATH=/usr/local/go/bin:$PATH\ncd /kaggle/working/subdomain-playground\n\necho \"==========================================\"\necho \"Building Security Tools (5-8 minutes)\"\necho \"==========================================\"\necho \"\"\n\nmkdir -p bin\n\n# Function to build Go tools\nbuild_tool() {\n    local name=$1\n    local repo=$2\n    local index=$3\n    local max_attempts=3\n    \n    echo \"[$index/5] Building $name...\"\n    \n    for attempt in $(seq 1 $max_attempts); do\n        if [ $attempt -gt 1 ]; then\n            echo \"  Retry $attempt/$max_attempts...\"\n            sleep 2\n        fi\n        \n        if GOBIN=$(pwd)/bin /usr/local/go/bin/go install -v ${repo}@latest 2>&1; then\n            if [ -f \"bin/$name\" ]; then\n                echo \"  ‚úì $name built successfully\"\n                return 0\n            fi\n        fi\n    done\n    \n    echo \"  ‚úó Failed to build $name\"\n    return 1\n}\n\n# Function to download precompiled binary\ndownload_binary() {\n    local name=$1\n    local url=$2\n    local index=$3\n    local max_attempts=3\n    \n    echo \"[$index/5] Downloading $name...\"\n    \n    for attempt in $(seq 1 $max_attempts); do\n        if [ $attempt -gt 1 ]; then\n            echo \"  Retry $attempt/$max_attempts...\"\n            sleep 2\n        fi\n        \n        if curl -sL \"$url\" -o /tmp/${name}.zip 2>&1; then\n            if unzip -q /tmp/${name}.zip -d bin/ 2>&1; then\n                if [ -f \"bin/$name\" ]; then\n                    chmod +x \"bin/$name\"\n                    echo \"  ‚úì $name downloaded successfully\"\n                    rm -f /tmp/${name}.zip\n                    return 0\n                fi\n            fi\n        fi\n    done\n    \n    echo \"  ‚ö†Ô∏è $name download failed (optional)\"\n    rm -f /tmp/${name}.zip\n    return 1\n}\n\n# Build tools\nbuild_tool \"subfinder\" \"github.com/projectdiscovery/subfinder/v2/cmd/subfinder\" \"1\"\necho \"\"\n\n# Download findomain (Rust-based, precompiled)\ndownload_binary \"findomain\" \"https://github.com/Findomain/Findomain/releases/latest/download/findomain-linux-i386.zip\" \"2\"\necho \"\"\n\nbuild_tool \"dnsx\" \"github.com/projectdiscovery/dnsx/cmd/dnsx\" \"3\"\necho \"\"\n\nbuild_tool \"httpx\" \"github.com/projectdiscovery/httpx/cmd/httpx\" \"4\"\necho \"\"\n\nbuild_tool \"subzy\" \"github.com/PentestPad/subzy\" \"5\"\necho \"\"\n\necho \"==========================================\"\necho \"Verification\"\necho \"==========================================\"\n\nREQUIRED=\"subfinder dnsx httpx subzy\"\nTOOLS_OK=true\n\nfor tool in $REQUIRED; do\n    if [ -f \"bin/$tool\" ]; then\n        echo \"  ‚úì bin/$tool\"\n    else\n        echo \"  ‚úó bin/$tool MISSING\"\n        TOOLS_OK=false\n    fi\ndone\n\n[ -f \"bin/findomain\" ] && echo \"  ‚úì bin/findomain (bonus!)\" || echo \"  ‚ö†Ô∏è bin/findomain (optional)\"\n\nif [ \"$TOOLS_OK\" = false ]; then\n    echo \"\"\n    echo \"‚ö†Ô∏è Required tools failed - re-run this cell\"\n    exit 1\nfi\n\necho \"\"\necho \"‚úì All required tools built!\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SUBFINDER_PATH'] = '/kaggle/working/subdomain-playground/bin/subfinder'\n",
    "os.environ['FINDOMAIN_PATH'] = '/kaggle/working/subdomain-playground/bin/findomain'\n",
    "os.environ['DNSX_PATH'] = '/kaggle/working/subdomain-playground/bin/dnsx'\n",
    "os.environ['HTTPX_PATH'] = '/kaggle/working/subdomain-playground/bin/httpx'\n",
    "os.environ['SUBZY_PATH'] = '/kaggle/working/subdomain-playground/bin/subzy'\n",
    "\n",
    "os.chdir('/kaggle/working/subdomain-playground')\n",
    "\n",
    "print(\"‚úì Environment configured\")\n",
    "print(f\"‚úì Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: Load and Filter Dataset\n\n**This cell:**\n1. Loads the CSV dataset\n2. Filters for Shopify stores\n3. Filters by HTTP status (403, 404, 409, 500, 503)\n4. Sorts by Est Monthly Page Views (descending)\n5. Shows you the top targets before scanning\n\n**You can adjust:**\n- `LIMIT`: How many domains to scan (default: 100)\n- `STATUS_CODES`: Which HTTP codes to filter (default: [403, 404, 409, 500, 503])\n- `SORT_BY`: Column to sort by (default: 'Est Monthly Page Views')\n\n**Available sort columns:**\n- `'Est Monthly Page Views'` (default - highest traffic)\n- `'Est. Monthly Sales'` (highest revenue)\n- `'Twitter Followers'`, `'YouTube Followers'`, `'Pinterest Followers'`, `'Tiktok Followers'`\n- `'Est. Products Sold'`\n- `'HTTP_Status'` (prioritize 403/404)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport os\n\n# ========================================\n# CONFIGURATION - ADJUST THESE\n# ========================================\nLIMIT = 100  # How many top domains to scan\nSTATUS_CODES = [403, 404, 409, 500, 503]  # HTTP status codes to target\nSORT_BY = 'Est Monthly Page Views'  # Exact column name from dataset\n\nprint(\"=\" * 80)\nprint(\"LOADING DATASET\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Load CSV\ncsv_path = '/kaggle/input/all-leads-merged/results.csv'\nprint(f\"Loading: {csv_path}\")\ndf = pd.read_csv(csv_path, low_memory=False)\nprint(f\"‚úì Loaded {len(df):,} total rows\")\nprint(\"\")\n\n# Filter for Shopify\nprint(\"Filtering for Shopify stores...\")\ndf_shopify = df[df['Is_Shopify'].astype(str).str.strip().str.lower() == 'yes'].copy()\nprint(f\"‚úì Shopify stores: {len(df_shopify):,}\")\n\n# Filter out *.myshopify.com\ndf_custom = df_shopify[~df_shopify['Subdomain'].astype(str).str.contains('myshopify.com', na=False)].copy()\nprint(f\"‚úì Custom domains (not *.myshopify.com): {len(df_custom):,}\")\nprint(\"\")\n\n# Filter by HTTP status\nprint(f\"Filtering by HTTP status: {STATUS_CODES}\")\ndf_filtered = df_custom[df_custom['HTTP_Status'].isin(STATUS_CODES)].copy()\nprint(f\"‚úì Matches: {len(df_filtered):,}\")\nprint(\"\")\n\n# Status breakdown\nprint(\"Status code breakdown:\")\nfor status in STATUS_CODES:\n    count = len(df_filtered[df_filtered['HTTP_Status'] == status])\n    print(f\"  {status}: {count:,}\")\nprint(\"\")\n\n# Sort by page views\nprint(f\"Sorting by: {SORT_BY} (highest first)\")\n\nif SORT_BY in df_filtered.columns:\n    # Convert to numeric (proper way to avoid warnings)\n    df_filtered = df_filtered.copy()\n    df_filtered['PageViews_Numeric'] = pd.to_numeric(\n        df_filtered[SORT_BY].astype(str).str.replace(r'[^\\d.]', '', regex=True),\n        errors='coerce'\n    ).fillna(0)\n    \n    # Sort and take top N\n    df_sorted = df_filtered.nlargest(LIMIT, 'PageViews_Numeric')\n    print(f\"‚úì Sorted {len(df_filtered):,} domains by {SORT_BY}\")\nelse:\n    print(f\"‚ö†Ô∏è Column '{SORT_BY}' not found\")\n    print(f\"Available columns: {', '.join(df_filtered.columns.tolist()[:10])}...\")\n    print(\"Using unsorted data\")\n    df_sorted = df_filtered.head(LIMIT)\n\nprint(\"\")\nprint(\"=\" * 80)\nprint(f\"TOP {LIMIT} TARGETS (Highest Traffic)\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Display top 20\ndisplay_cols = ['Subdomain', 'HTTP_Status', SORT_BY, 'CNAME_Record']\ndisplay_cols = [col for col in display_cols if col in df_sorted.columns]\n\nprint(\"Top 20:\")\nprint(df_sorted[display_cols].head(20).to_string(index=False))\nprint(\"\")\nif len(df_sorted) > 20:\n    print(f\"... and {len(df_sorted) - 20} more\")\nprint(\"\")\n\n# Export to file\ntargets_file = '/kaggle/working/scan_targets.txt'\ndf_sorted['Subdomain'].to_csv(targets_file, index=False, header=False)\nprint(f\"‚úì Exported {len(df_sorted)} targets to: {targets_file}\")\nprint(\"\")\n\n# Also save CSV for reference\ncsv_file = '/kaggle/working/filtered_targets.csv'\ndf_sorted[display_cols].to_csv(csv_file, index=False)\nprint(f\"‚úì Saved filtered data to: {csv_file}\")\nprint(\"\")\nprint(\"=\" * 80)\nprint(\"Next: Run Cell 7 to scan these targets\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Deep Scanner Validation (10-15 minutes)\n",
    "\n",
    "**Scans the filtered targets with:**\n",
    "- DNS validation (CNAME chains)\n",
    "- HTTP validation (body analysis)\n",
    "- Takeover detection (subzy)\n",
    "- CNAME blacklist filtering (46 patterns)\n",
    "- Cloudflare verification detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DEEP SCANNER VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "print(\"Scanning: /kaggle/working/scan_targets.txt\")\n",
    "print(\"Mode: quick (skips enumeration - already subdomains)\")\n",
    "print(\"Workers: 2 (optimized for Kaggle)\")\n",
    "print(\"\")\n",
    "print(\"Features:\")\n",
    "print(\"  ‚úì Automatic subdomain detection\")\n",
    "print(\"  ‚úì CNAME blacklist (46 patterns)\")\n",
    "print(\"  ‚úì Cloudflare verification detection\")\n",
    "print(\"  ‚úì Custom DNS resolvers\")\n",
    "print(\"  ‚úì Tools: subfinder + findomain + dnsx + httpx + subzy\")\n",
    "print(\"\")\n",
    "\n",
    "scan_start = time.time()\n",
    "\n",
    "scan_cmd = [\n",
    "    sys.executable, '-u', 'scan.py',\n",
    "    '-l', '/kaggle/working/scan_targets.txt',\n",
    "    '--mode', 'quick',\n",
    "    '--provider', 'Shopify',\n",
    "    '--require-cname',\n",
    "    '--filter-status', '3*,4*,5*',\n",
    "    '--workers', '2',\n",
    "    '--json',\n",
    "    '--output', '/kaggle/working/takeover_results.json'\n",
    "]\n",
    "\n",
    "scan_proc = subprocess.Popen(\n",
    "    scan_cmd,\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.STDOUT,\n",
    "    universal_newlines=True,\n",
    "    bufsize=1\n",
    ")\n",
    "\n",
    "for line in iter(scan_proc.stdout.readline, ''):\n",
    "    if line:\n",
    "        print(line, end='', flush=True)\n",
    "\n",
    "scan_proc.wait()\n",
    "scan_time = time.time() - scan_start\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úì Scan Complete in {scan_time/60:.1f} minutes\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "print(\"‚úì Results: /kaggle/working/takeover_results.json\")\n",
    "print(\"\")\n",
    "print(\"Next: Run Cell 8 to view results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: View Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "results_file = Path('/kaggle/working/takeover_results.json')\n",
    "\n",
    "if not results_file.exists():\n",
    "    print(\"‚ùå Results file not found!\")\n",
    "    print(\"Make sure Cell 7 completed successfully\")\n",
    "else:\n",
    "    with open(results_file) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"SCAN RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\")\n",
    "    print(f\"Total analyzed: {len(results)}\")\n",
    "    \n",
    "    definite = [r for r in results if 'DEFINITE' in r.get('evidence', '')]\n",
    "    high_prob = [r for r in results if 'HIGH PROBABILITY' in r.get('evidence', '')]\n",
    "    false_pos = [r for r in results if 'FALSE POSITIVE' in r.get('evidence', '')]\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"Breakdown:\")\n",
    "    print(f\"  üî¥ DEFINITE TAKEOVER: {len(definite)}\")\n",
    "    print(f\"  ‚ö†Ô∏è  HIGH PROBABILITY: {len(high_prob)}\")\n",
    "    print(f\"  ‚ùå FALSE POSITIVE: {len(false_pos)}\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TOP 10 VULNERABILITIES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if definite:\n",
    "        df = pd.DataFrame(definite)\n",
    "        df_sorted = df.sort_values('confidence', ascending=False) if 'confidence' in df.columns else df\n",
    "        \n",
    "        for i, row in df_sorted.head(10).iterrows():\n",
    "            print(\"\")\n",
    "            print(f\"{i+1}. {row['subdomain']}\")\n",
    "            print(f\"   Status: {row.get('status', 'N/A')}\")\n",
    "            print(f\"   CNAME: {row.get('cname', 'N/A')}\")\n",
    "            print(f\"   Evidence: {row.get('evidence', 'N/A')}\")\n",
    "            print(f\"   Confidence: {row.get('confidence', 0)}\")\n",
    "    else:\n",
    "        print(\"\")\n",
    "        print(\"No definite takeovers found.\")\n",
    "        print(\"Check HIGH PROBABILITY results.\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Next: Run Cell 9 to export CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: Export Results to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "results_file = Path('/kaggle/working/takeover_results.json')\n",
    "\n",
    "if results_file.exists():\n",
    "    with open(results_file) as f:\n",
    "        results = json.load(f)\n",
    "    \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    columns = ['subdomain', 'status', 'cname', 'evidence', 'confidence', 'message']\n",
    "    df_export = df[[col for col in columns if col in df.columns]]\n",
    "    \n",
    "    if 'confidence' in df_export.columns:\n",
    "        df_export = df_export.sort_values('confidence', ascending=False)\n",
    "    \n",
    "    output_csv = Path('/kaggle/working/shopify_takeovers.csv')\n",
    "    df_export.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print(\"CSV EXPORT\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\")\n",
    "    print(f\"‚úì Exported {len(df_export)} results to: {output_csv}\")\n",
    "    print(\"\")\n",
    "    print(\"Preview (top 20):\")\n",
    "    print(df_export.head(20).to_string(index=False))\n",
    "    \n",
    "    if len(df_export) > 20:\n",
    "        print(\"\")\n",
    "        print(f\"... and {len(df_export) - 20} more rows\")\n",
    "    \n",
    "    df_high = df_export[df_export['evidence'].str.contains('DEFINITE|HIGH', na=False)]\n",
    "    if len(df_high) > 0:\n",
    "        high_risk_csv = Path('/kaggle/working/shopify_high_risk.csv')\n",
    "        df_high.to_csv(high_risk_csv, index=False)\n",
    "        print(\"\")\n",
    "        print(f\"‚úì High-risk only ({len(df_high)} results): {high_risk_csv}\")\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ COMPLETE!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\")\n",
    "    print(\"Download files from Kaggle Output section\")\n",
    "else:\n",
    "    print(\"‚ùå Results file not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 10: Performance Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PERFORMANCE SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "print(\"Workflow:\")\n",
    "print(\"  1. Load 1.7M row CSV dataset\")\n",
    "print(\"  2. Filter by Shopify + HTTP status\")\n",
    "print(\"  3. Sort by Est Monthly Page Views\")\n",
    "print(\"  4. Scan top N targets (default: 100)\")\n",
    "print(\"\")\n",
    "print(\"Time:\")\n",
    "print(\"  Setup (Cells 1-5): 6-10 minutes\")\n",
    "print(\"  Filter (Cell 6): 2-5 seconds\")\n",
    "print(\"  Scan (Cell 7): 10-15 minutes\")\n",
    "print(\"  Total: 15-25 minutes\")\n",
    "print(\"\")\n",
    "print(\"Tools:\")\n",
    "print(\"  ‚úì subfinder (passive enumeration)\")\n",
    "print(\"  ‚úì findomain (bonus coverage)\")\n",
    "print(\"  ‚úì dnsx (DNS + CNAME)\")\n",
    "print(\"  ‚úì httpx (HTTP validation)\")\n",
    "print(\"  ‚úì subzy (takeover detection)\")\n",
    "print(\"\")\n",
    "print(\"Features:\")\n",
    "print(\"  ‚úì Dataset-based (no Google Sheets)\")\n",
    "print(\"  ‚úì Filter by HTTP status\")\n",
    "print(\"  ‚úì Sort by page views\")\n",
    "print(\"  ‚úì CNAME blacklist (46 patterns)\")\n",
    "print(\"  ‚úì Cloudflare verification detection\")\n",
    "print(\"  ‚úì Custom DNS resolvers\")\n",
    "print(\"\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}