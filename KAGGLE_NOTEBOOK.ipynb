{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Shopify Subdomain Takeover Scanner\n\n**Dataset-based workflow:**\n1. Load CSV dataset (`/kaggle/input/all-leads-merged/results.csv`)\n2. Sort by Est Monthly Page Views (highest traffic first)\n3. Deep scan with auto-resume support\n\n**Features:**\n- âœ… Auto-resume: Progress saved every 50 domains\n- âœ… Results backup: Previous results preserved on pull/clone\n- âœ… Persistent storage: Progress and results survive notebook restarts\n\n**Files stored in `/kaggle/working/` (persist between sessions):**\n- `scan_progress.json` - Tracks last scanned row\n- `all_results.json` - Accumulated scan results"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 1: Backup Previous Results (Run Before Clone)\n\n**This cell backs up your scan results before cloning overwrites them.**\n\nResults are saved to `/kaggle/working/all_results.json` which persists outside the repo folder."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport shutil\nimport os\nfrom pathlib import Path\n\n# Paths\nREPO_RESULTS = '/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json'\nBACKUP_RESULTS = '/kaggle/working/all_results.json'\n\nprint(\"=\" * 80)\nprint(\"BACKING UP RESULTS\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Check if there are results to backup\nif os.path.exists(REPO_RESULTS):\n    # Load current results from repo\n    with open(REPO_RESULTS, 'r') as f:\n        current_results = json.load(f)\n    \n    print(f\"Found {len(current_results)} results in repo folder\")\n    \n    # Load existing backup if any\n    existing_backup = []\n    if os.path.exists(BACKUP_RESULTS):\n        with open(BACKUP_RESULTS, 'r') as f:\n            existing_backup = json.load(f)\n        print(f\"Found {len(existing_backup)} results in backup\")\n    \n    # Merge (avoid duplicates by subdomain)\n    existing_subdomains = {r['subdomain'] for r in existing_backup}\n    new_results = [r for r in current_results if r['subdomain'] not in existing_subdomains]\n    \n    merged = existing_backup + new_results\n    \n    # Save merged backup\n    with open(BACKUP_RESULTS, 'w') as f:\n        json.dump(merged, f, indent=2)\n    \n    print(f\"Added {len(new_results)} new results\")\n    print(f\"âœ“ Total backed up: {len(merged)} results\")\n    print(f\"âœ“ Saved to: {BACKUP_RESULTS}\")\nelse:\n    if os.path.exists(BACKUP_RESULTS):\n        with open(BACKUP_RESULTS, 'r') as f:\n            existing = json.load(f)\n        print(f\"No new results in repo folder\")\n        print(f\"âœ“ Existing backup preserved: {len(existing)} results\")\n    else:\n        print(\"No results to backup yet (first run)\")\n\nprint(\"\")\nprint(\"=\" * 80)\nprint(\"Safe to clone/pull now - results are backed up!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 2: Clone Project from GitHub"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%bash\nWORKDIR=\"/kaggle/working\"\nPROJECT_DIR=\"$WORKDIR/subdomain-playground\"\n\necho \"==========================================\"\necho \"Cloning Project from GitHub\"\necho \"==========================================\"\n\nmkdir -p \"$WORKDIR\"\ncd \"$WORKDIR\"\n\nif [ -d \"$PROJECT_DIR\" ]; then\n    echo \"Removing existing copy...\"\n    rm -rf \"$PROJECT_DIR\"\nfi\n\ngit clone --depth 1 https://github.com/sayihhamza/subdomain-playground.git subdomain-playground\n\nif [ ! -d \"$PROJECT_DIR\" ]; then\n    echo \"âœ— Clone failed!\"\n    exit 1\nfi\n\ncd \"$PROJECT_DIR\"\n\n# Create data/scans directory for results\nmkdir -p data/scans\n\necho \"\"\necho \"âœ“ Project cloned successfully!\"\necho \"\"\necho \"Project structure:\"\nls -lh | head -20"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 3: Install Go 1.24"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tmp\n",
    "\n",
    "if command -v sudo >/dev/null 2>&1; then\n",
    "    SUDO=\"sudo\"\n",
    "else\n",
    "    SUDO=\"\"\n",
    "fi\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Installing Go 1.24.1\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "$SUDO rm -rf /usr/local/go 2>/dev/null || true\n",
    "\n",
    "echo \"Downloading Go 1.24.1...\"\n",
    "for i in {1..3}; do\n",
    "    wget -q https://go.dev/dl/go1.24.1.linux-amd64.tar.gz -O /tmp/go.tar.gz && break || sleep 5\n",
    "done\n",
    "\n",
    "if [ ! -f /tmp/go.tar.gz ]; then\n",
    "    echo \"âœ— Failed to download Go\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "$SUDO tar -C /usr/local -xzf /tmp/go.tar.gz\n",
    "rm -f /tmp/go.tar.gz\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Go 1.24.1 installed!\"\n",
    "/usr/local/go/bin/go version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 4: Build Security Tools\n\nBuilds 5 tools (5-8 minutes):\n- **subfinder**: Passive subdomain enumeration\n- **findomain**: Additional passive enumeration (optional)\n- **httpx**: HTTP validation\n- **dnsx**: DNS + CNAME validation\n- **subzy**: Takeover detection"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=/usr/local/go/bin:$PATH\n",
    "cd /kaggle/working/subdomain-playground\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Building Security Tools\"\n",
    "echo \"==========================================\"\n",
    "echo \"\"\n",
    "\n",
    "mkdir -p bin\n",
    "\n",
    "# Build Go tools\n",
    "build_tool() {\n",
    "    local name=$1\n",
    "    local repo=$2\n",
    "    local index=$3\n",
    "    local total=$4\n",
    "    local max_attempts=3\n",
    "    \n",
    "    echo \"[$index/$total] Building $name...\"\n",
    "    \n",
    "    for attempt in $(seq 1 $max_attempts); do\n",
    "        if [ $attempt -gt 1 ]; then\n",
    "            echo \"  Retry $attempt/$max_attempts...\"\n",
    "            sleep 2\n",
    "        fi\n",
    "        \n",
    "        if GOBIN=$(pwd)/bin /usr/local/go/bin/go install -v ${repo}@latest 2>&1; then\n",
    "            if [ -f \"bin/$name\" ]; then\n",
    "                echo \"  âœ“ $name built successfully\"\n",
    "                return 0\n",
    "            fi\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    echo \"  âœ— Failed to build $name\"\n",
    "    return 1\n",
    "}\n",
    "\n",
    "# Download precompiled binary\n",
    "download_binary() {\n",
    "    local name=$1\n",
    "    local url=$2\n",
    "    local index=$3\n",
    "    local total=$4\n",
    "    local max_attempts=3\n",
    "    \n",
    "    echo \"[$index/$total] Downloading $name...\"\n",
    "    \n",
    "    for attempt in $(seq 1 $max_attempts); do\n",
    "        if [ $attempt -gt 1 ]; then\n",
    "            echo \"  Retry $attempt/$max_attempts...\"\n",
    "            sleep 2\n",
    "        fi\n",
    "        \n",
    "        if curl -sL \"$url\" -o /tmp/${name}.zip 2>&1; then\n",
    "            if unzip -q /tmp/${name}.zip -d bin/ 2>&1; then\n",
    "                if [ -f \"bin/$name\" ]; then\n",
    "                    chmod +x \"bin/$name\"\n",
    "                    echo \"  âœ“ $name downloaded successfully\"\n",
    "                    rm -f /tmp/${name}.zip\n",
    "                    return 0\n",
    "                fi\n",
    "            fi\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    echo \"  âš ï¸  $name download failed (optional)\"\n",
    "    rm -f /tmp/${name}.zip\n",
    "    return 1\n",
    "}\n",
    "\n",
    "# Build required tools\n",
    "build_tool \"subfinder\" \"github.com/projectdiscovery/subfinder/v2/cmd/subfinder\" \"1\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "download_binary \"findomain\" \"https://github.com/Findomain/Findomain/releases/latest/download/findomain-linux-i386.zip\" \"2\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"httpx\" \"github.com/projectdiscovery/httpx/cmd/httpx\" \"3\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"dnsx\" \"github.com/projectdiscovery/dnsx/cmd/dnsx\" \"4\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"subzy\" \"github.com/PentestPad/subzy\" \"5\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Verification\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "REQUIRED=\"subfinder httpx dnsx subzy\"\n",
    "TOOLS_OK=true\n",
    "\n",
    "for tool in $REQUIRED; do\n",
    "    if [ -f \"bin/$tool\" ]; then\n",
    "        echo \"  âœ“ bin/$tool\"\n",
    "    else\n",
    "        echo \"  âœ— bin/$tool MISSING\"\n",
    "        TOOLS_OK=false\n",
    "    fi\n",
    "done\n",
    "\n",
    "[ -f \"bin/findomain\" ] && echo \"  âœ“ bin/findomain (bonus!)\" || echo \"  âš ï¸  bin/findomain (optional)\"\n",
    "\n",
    "if [ \"$TOOLS_OK\" = false ]; then\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸  Required tools failed - re-run this cell\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ All required tools built!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 5: Set Environment Variables"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SUBFINDER_PATH'] = '/kaggle/working/subdomain-playground/bin/subfinder'\n",
    "os.environ['FINDOMAIN_PATH'] = '/kaggle/working/subdomain-playground/bin/findomain'\n",
    "os.environ['DNSX_PATH'] = '/kaggle/working/subdomain-playground/bin/dnsx'\n",
    "os.environ['HTTPX_PATH'] = '/kaggle/working/subdomain-playground/bin/httpx'\n",
    "os.environ['SUBZY_PATH'] = '/kaggle/working/subdomain-playground/bin/subzy'\n",
    "\n",
    "os.chdir('/kaggle/working/subdomain-playground')\n",
    "\n",
    "print(\"âœ“ Environment configured\")\n",
    "print(f\"âœ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 6: Load and Sort Dataset\n\n**Configuration:**\n- `SORT_BY`: Column to sort by (default: 'Est Monthly Page Views')\n\n**What this cell does:**\n1. Loads the full CSV dataset from Kaggle input\n2. Sorts by specified column (descending)\n3. Creates `df_base_filtered` for Cell 7 and Cell 8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# ========================================\n# CONFIGURATION\n# ========================================\nSORT_BY = 'Est Monthly Page Views'  # Column to sort by\n\nprint(\"=\" * 80)\nprint(\"LOADING AND SORTING DATASET\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Load CSV from Kaggle input\ncsv_path = '/kaggle/input/all-leads-merged/results.csv'\nprint(f\"Loading: {csv_path}\")\ndf = pd.read_csv(csv_path, low_memory=False)\nprint(f\"âœ“ Loaded {len(df):,} total rows\")\nprint(\"\")\n\n# Sort by specified column (NO FILTERING)\nprint(f\"Sorting by: {SORT_BY} (descending)\")\nif SORT_BY in df.columns:\n    # Convert to numeric if it's a numeric column\n    df['_sort_numeric'] = pd.to_numeric(\n        df[SORT_BY].astype(str).str.replace(r'[^\\d.]', '', regex=True),\n        errors='coerce'\n    ).fillna(0)\n    df_sorted = df.sort_values('_sort_numeric', ascending=False)\n    print(f\"âœ“ Sorted by {SORT_BY}\")\nelse:\n    print(f\"âš ï¸  Column '{SORT_BY}' not found - using unsorted\")\n    df_sorted = df\nprint(\"\")\n\n# Store base dataset for Cell 7 (preview) and Cell 8 (scan)\ndf_base_filtered = df_sorted.copy()\n\nprint(\"=\" * 80)\nprint(\"âœ“ Dataset loaded and sorted\")\nprint(f\"Total available: {len(df_base_filtered):,} rows\")\nprint(\"\")\nprint(\"Next steps:\")\nprint(\"  â€¢ Cell 7: Apply filters + preview range\")\nprint(\"  â€¢ Cell 8: Apply filters + scan range\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 7: Preview Dataset (Optional)\n\n**Configure preview range:**\n- `PREVIEW_START_ROW` and `PREVIEW_END_ROW`\n\n**Filters applied (for preview only):**\n1. Is_Shopify == 'Yes'\n2. Exclude *.myshopify.com domains\n3. Exclude HTTP 200 and 429"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ========================================\n",
    "# PREVIEW CONFIGURATION (Cell 7 only)\n",
    "# ========================================\n",
    "PREVIEW_START_ROW = 1       # First row to preview (1-indexed)\n",
    "PREVIEW_END_ROW = 100       # Last row to preview\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(f\"PREVIEW: Rows {PREVIEW_START_ROW} to {PREVIEW_END_ROW} from Sorted Dataset\")\n",
    "print(\"=\" * 120)\n",
    "print(\"\")\n",
    "print(f\"Source: df_base_filtered from Cell 6 (sorted by {SORT_BY})\")\n",
    "print(f\"Total available: {len(df_base_filtered):,} rows\")\n",
    "print(\"\")\n",
    "\n",
    "# Select preview range from base dataset (sorted, but unfiltered)\n",
    "df_preview = df_base_filtered.iloc[PREVIEW_START_ROW-1:PREVIEW_END_ROW].copy()\n",
    "print(f\"Selected range: {len(df_preview)} rows\")\n",
    "print(\"\")\n",
    "\n",
    "# Apply preview filters (for display only)\n",
    "print(\"Applying filters for preview...\")\n",
    "\n",
    "# Filter A: Shopify stores only\n",
    "is_shopify = df_preview['Is_Shopify'] == 'Yes'\n",
    "print(f\"  After Shopify filter: {is_shopify.sum()} rows\")\n",
    "\n",
    "# Filter B: Exclude raw 'myshopify.com' domains\n",
    "is_custom_domain = ~df_preview['Subdomain'].str.contains('myshopify.com', case=False, na=False)\n",
    "print(f\"  After myshopify.com exclusion: {(is_shopify & is_custom_domain).sum()} rows\")\n",
    "\n",
    "# Filter C: Exclude HTTP 200 and 429\n",
    "excluded_statuses = [200, 429, '200', '429', 200.0, 429.0]\n",
    "is_interesting_status = ~df_preview['HTTP_Status'].isin(excluded_statuses)\n",
    "print(f\"  After status exclusion: {(is_shopify & is_custom_domain & is_interesting_status).sum()} rows\")\n",
    "\n",
    "# Apply all filters\n",
    "df_preview_filtered = df_preview[is_shopify & is_custom_domain & is_interesting_status].copy()\n",
    "print(\"\")\n",
    "\n",
    "# Prepare display with original row numbers\n",
    "display_df = df_preview_filtered.copy()\n",
    "# Calculate actual row numbers from base dataset\n",
    "actual_rows = []\n",
    "for idx in display_df.index:\n",
    "    actual_row = df_base_filtered.index.get_loc(idx) + 1\n",
    "    actual_rows.append(actual_row)\n",
    "display_df.insert(0, 'Row', actual_rows)\n",
    "\n",
    "# Select columns to display\n",
    "display_columns = ['Row', 'Subdomain', 'HTTP_Status', SORT_BY, 'CNAME_Record']\n",
    "display_columns = [col for col in display_columns if col in display_df.columns]\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(f\"Displaying {len(display_df)} domains after filters:\")\n",
    "print(\"=\" * 120)\n",
    "print(\"\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Display\n",
    "print(display_df[display_columns].to_string(index=False))\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 120)\n",
    "print(\"ðŸ“‹ Preview Summary:\")\n",
    "print(f\"  â€¢ Preview range: Rows {PREVIEW_START_ROW} to {PREVIEW_END_ROW}\")\n",
    "print(f\"  â€¢ After filters: {len(display_df)} domains\")\n",
    "print(f\"  â€¢ Sorted by: {SORT_BY} (highest first)\")\n",
    "print(f\"  â€¢ Filters: Shopify + Custom domain + Exclude status 200/429\")\n",
    "print(\"\")\n",
    "print(\"ðŸ’¡ NOTE: Cell 8 scans ALL domains in range (scan.py applies its own filters)\")\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8: Deep Scan with Auto-Resume\n\n**How it works:**\n1. Progress saved to `/kaggle/working/scan_progress.json` every 50 domains\n2. Results saved to `/kaggle/working/all_results.json` after each session\n3. On re-run, automatically resumes from last completed row\n\n**Configuration:**\n- `SCAN_END_ROW` - Last row to scan (default: all rows)\n- `FORCE_RESTART = True` - Start from row 1, ignoring saved progress\n\n**Manual Resume (if needed):**\n```python\nimport json\nwith open('/kaggle/working/scan_progress.json', 'w') as f:\n    json.dump({'last_row': 2880, 'total_scanned': 2880}, f)\n```"
  },
  {
   "cell_type": "code",
   "source": "import subprocess\nimport sys\nimport pandas as pd\nimport os\nimport json\nimport re\nfrom pathlib import Path\nfrom datetime import datetime\n\n# ========================================\n# SCAN CONFIGURATION\n# ========================================\nSCAN_END_ROW = 1786654       # Last row to scan\nFORCE_RESTART = False        # Set to True to start from row 1\n\n# ========================================\n# FILE PATHS (persist in /kaggle/working)\n# ========================================\nPROGRESS_FILE = '/kaggle/working/scan_progress.json'\nBACKUP_RESULTS = '/kaggle/working/all_results.json'\nREPO_RESULTS = '/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json'\n\ndef load_progress():\n    \"\"\"Load last scanned row from progress file\"\"\"\n    if os.path.exists(PROGRESS_FILE):\n        try:\n            with open(PROGRESS_FILE, 'r') as f:\n                return json.load(f)\n        except:\n            pass\n    return {'last_row': 0, 'total_scanned': 0}\n\ndef save_progress(last_row, total_scanned):\n    \"\"\"Save progress to file\"\"\"\n    with open(PROGRESS_FILE, 'w') as f:\n        json.dump({\n            'last_row': last_row,\n            'total_scanned': total_scanned,\n            'updated': datetime.now().isoformat()\n        }, f)\n\ndef backup_results():\n    \"\"\"Merge repo results into backup file\"\"\"\n    if not os.path.exists(REPO_RESULTS):\n        return\n    \n    # Load current results\n    with open(REPO_RESULTS, 'r') as f:\n        current = json.load(f)\n    \n    if not current:\n        return\n    \n    # Load existing backup\n    existing = []\n    if os.path.exists(BACKUP_RESULTS):\n        with open(BACKUP_RESULTS, 'r') as f:\n            existing = json.load(f)\n    \n    # Merge (avoid duplicates)\n    existing_subs = {r['subdomain'] for r in existing}\n    new = [r for r in current if r['subdomain'] not in existing_subs]\n    merged = existing + new\n    \n    # Save\n    with open(BACKUP_RESULTS, 'w') as f:\n        json.dump(merged, f, indent=2)\n    \n    return len(merged), len(new)\n\nprint(\"=\" * 80)\nprint(\"SCAN WITH AUTO-RESUME\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Determine starting row\nif FORCE_RESTART:\n    print(\"ðŸ”„ FORCE_RESTART = True â†’ Starting from row 1\")\n    if os.path.exists(PROGRESS_FILE):\n        os.remove(PROGRESS_FILE)\n    start_row = 1\n    total_scanned = 0\nelse:\n    progress = load_progress()\n    start_row = progress['last_row'] + 1\n    total_scanned = progress['total_scanned']\n    if start_row > 1:\n        print(f\"ðŸ“‚ Resuming from saved progress!\")\n        print(f\"   Previously scanned: {total_scanned:,} domains\")\n        print(f\"   Last completed row: {progress['last_row']:,}\")\n        print(f\"   â†’ Starting from row: {start_row:,}\")\n    else:\n        print(\"ðŸ“ No previous progress â†’ Starting from row 1\")\nprint(\"\")\n\n# Show backup status\nif os.path.exists(BACKUP_RESULTS):\n    with open(BACKUP_RESULTS, 'r') as f:\n        backup_count = len(json.load(f))\n    print(f\"ðŸ“¦ Backup contains: {backup_count} results\")\n    print(\"\")\n\n# Check if already complete\nif start_row > SCAN_END_ROW:\n    print(\"âœ… SCAN COMPLETE!\")\n    print(f\"   All {SCAN_END_ROW:,} rows have been processed.\")\n    print(f\"   Total scanned: {total_scanned:,}\")\n    print(\"\")\n    print(\"To restart, set FORCE_RESTART = True\")\nelse:\n    print(f\"Source: df_base_filtered (sorted by {SORT_BY})\")\n    print(f\"Total in dataset: {len(df_base_filtered):,} rows\")\n    print(f\"Scan range: Row {start_row:,} to {SCAN_END_ROW:,}\")\n    print(\"\")\n\n    # Select remaining rows\n    df_scan = df_base_filtered.iloc[start_row-1:SCAN_END_ROW].copy()\n    domains_this_session = len(df_scan)\n    print(f\"Domains this session: {domains_this_session:,}\")\n    print(\"\")\n\n    # Create data directory and save targets\n    data_dir = Path('/kaggle/working/subdomain-playground/data')\n    data_dir.mkdir(parents=True, exist_ok=True)\n    scans_dir = data_dir / 'scans'\n    scans_dir.mkdir(exist_ok=True)\n    \n    targets_file = data_dir / 'all_sources.txt'\n    df_scan['Subdomain'].to_csv(targets_file, index=False, header=False)\n    print(f\"âœ“ Saved to: {targets_file}\")\n    print(\"\")\n\n    # Setup environment\n    os.chdir('/kaggle/working/subdomain-playground')\n    os.environ['PATH'] = f\"/kaggle/working/subdomain-playground/bin:{os.environ['PATH']}\"\n    os.environ['SUBFINDER_PATH'] = '/kaggle/working/subdomain-playground/bin/subfinder'\n    os.environ['FINDOMAIN_PATH'] = '/kaggle/working/subdomain-playground/bin/findomain'\n    os.environ['DNSX_PATH'] = '/kaggle/working/subdomain-playground/bin/dnsx'\n    os.environ['HTTPX_PATH'] = '/kaggle/working/subdomain-playground/bin/httpx'\n    os.environ['SUBZY_PATH'] = '/kaggle/working/subdomain-playground/bin/subzy'\n\n    print(\"=\" * 80)\n    print(\"STARTING SCAN\")\n    print(\"=\" * 80)\n    print(\"\")\n\n    # Run scan\n    process = subprocess.Popen(\n        [sys.executable, '-u', 'scan.py',\n         '-l', 'data/all_sources.txt',\n         '--mode', 'full',\n         '--require-cname-contains', 'shopify',\n         '--filter-status', '3*,4*,5*',\n         '--workers', '2'],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        universal_newlines=True,\n        bufsize=1\n    )\n\n    # Track progress from output\n    progress_pattern = re.compile(r'\\[(\\d+)/(\\d+)\\]')\n    domains_completed = 0\n    \n    for line in process.stdout:\n        print(line, end='', flush=True)\n        \n        # Parse [X/Y] progress\n        match = progress_pattern.search(line)\n        if match:\n            domains_completed = int(match.group(1))\n            \n            # Save progress every 50 domains\n            if domains_completed % 50 == 0:\n                current_row = start_row + domains_completed - 1\n                save_progress(current_row, total_scanned + domains_completed)\n                # Also backup results periodically\n                if domains_completed % 200 == 0:\n                    backup_results()\n\n    process.wait()\n    \n    # Final save\n    final_row = start_row + domains_completed - 1 if domains_completed > 0 else start_row - 1\n    final_total = total_scanned + domains_completed\n    save_progress(final_row, final_total)\n    \n    # Backup results\n    backup_info = backup_results()\n    \n    print(\"\")\n    print(\"=\" * 80)\n    print(f\"Session complete!\")\n    print(f\"   Domains this session: {domains_completed:,}\")\n    print(f\"   Total scanned so far: {final_total:,}\")\n    print(f\"   Last row completed: {final_row:,}\")\n    print(f\"   Remaining: {SCAN_END_ROW - final_row:,} rows\")\n    if backup_info:\n        print(f\"   Results backed up: {backup_info[0]:,} total ({backup_info[1]} new)\")\n    print(\"\")\n    print(\"Progress and results saved. Re-run this cell to continue.\")\n    print(\"=\" * 80)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9: View Results\n\nViews results from the backup file (`/kaggle/working/all_results.json`) which persists across sessions."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport os\nfrom pathlib import Path\n\n# Try backup file first, then repo file\nBACKUP_RESULTS = '/kaggle/working/all_results.json'\nREPO_RESULTS = '/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json'\n\nresults = []\nresults_file = None\n\nif os.path.exists(BACKUP_RESULTS):\n    results_file = BACKUP_RESULTS\n    with open(BACKUP_RESULTS, 'r') as f:\n        results = json.load(f)\nelif os.path.exists(REPO_RESULTS):\n    results_file = REPO_RESULTS\n    with open(REPO_RESULTS, 'r') as f:\n        results = json.load(f)\n\nif results:\n    print(\"=\" * 80)\n    print(\"SCAN RESULTS\")\n    print(\"=\" * 80)\n    print(\"\")\n    print(f\"Source: {results_file}\")\n    print(f\"Total candidates found: {len(results)}\")\n\n    # Count by risk level\n    risk_counts = {}\n    for r in results:\n        risk = r.get(\"risk_level\", \"unknown\")\n        risk_counts[risk] = risk_counts.get(risk, 0) + 1\n\n    print(\"\")\n    print(\"Breakdown by risk:\")\n    for risk, count in sorted(risk_counts.items()):\n        print(f\"  {risk.upper()}: {count}\")\n\n    print(\"\")\n    print(\"=\" * 80)\n    print(\"TOP 20 FINDINGS\")\n    print(\"=\" * 80)\n\n    sorted_results = sorted(results, key=lambda x: x.get(\"confidence_score\", 0), reverse=True)\n\n    for i, r in enumerate(sorted_results[:20], 1):\n        print(\"\")\n        print(f\"{i}. {r['subdomain']}\")\n        print(f\"   CNAME: {r.get('cname', 'N/A')}\")\n        print(f\"   HTTP Status: {r.get('http_status', 'N/A')}\")\n        print(f\"   Risk: {r.get('risk_level', 'N/A')}\")\n        print(f\"   Confidence: {r.get('confidence_score', 0)}\")\nelse:\n    print(\"No results found yet.\")\n    print(\"Run Cell 9 (scan) first.\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}