{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shopify Subdomain Takeover Scanner\n",
    "\n",
    "**Dataset-based workflow:**\n",
    "1. Load CSV dataset (`/kaggle/input/all-leads-merged/results.csv`)\n",
    "2. Sort by Est Monthly Page Views (highest traffic first)\n",
    "3. Deep scan with auto-resume support\n",
    "\n",
    "**Features:**\n",
    "- âœ… Auto-resume: Progress saved every 50 domains\n",
    "- âœ… Results backup: Previous results preserved on pull/clone\n",
    "- âœ… Persistent storage: Progress and results survive notebook restarts\n",
    "\n",
    "**Files stored in `/kaggle/working/` (persist between sessions):**\n",
    "- `scan_progress.json` - Tracks last scanned row\n",
    "- `all_results.json` - Accumulated scan results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Backup Previous Results (Run Before Clone)\n",
    "\n",
    "**This cell backs up your scan results before cloning overwrites them.**\n",
    "\n",
    "Results are saved to `/kaggle/working/all_results.json` which persists outside the repo folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "REPO_RESULTS = '/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json'\n",
    "BACKUP_RESULTS = '/kaggle/working/all_results.json'\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"BACKING UP RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "\n",
    "# Check if there are results to backup\n",
    "if os.path.exists(REPO_RESULTS):\n",
    "    # Load current results from repo\n",
    "    with open(REPO_RESULTS, 'r') as f:\n",
    "        current_results = json.load(f)\n",
    "    \n",
    "    print(f\"Found {len(current_results)} results in repo folder\")\n",
    "    \n",
    "    # Load existing backup if any\n",
    "    existing_backup = []\n",
    "    if os.path.exists(BACKUP_RESULTS):\n",
    "        with open(BACKUP_RESULTS, 'r') as f:\n",
    "            existing_backup = json.load(f)\n",
    "        print(f\"Found {len(existing_backup)} results in backup\")\n",
    "    \n",
    "    # Merge (avoid duplicates by subdomain)\n",
    "    existing_subdomains = {r['subdomain'] for r in existing_backup}\n",
    "    new_results = [r for r in current_results if r['subdomain'] not in existing_subdomains]\n",
    "    \n",
    "    merged = existing_backup + new_results\n",
    "    \n",
    "    # Save merged backup\n",
    "    with open(BACKUP_RESULTS, 'w') as f:\n",
    "        json.dump(merged, f, indent=2)\n",
    "    \n",
    "    print(f\"Added {len(new_results)} new results\")\n",
    "    print(f\"âœ“ Total backed up: {len(merged)} results\")\n",
    "    print(f\"âœ“ Saved to: {BACKUP_RESULTS}\")\n",
    "else:\n",
    "    if os.path.exists(BACKUP_RESULTS):\n",
    "        with open(BACKUP_RESULTS, 'r') as f:\n",
    "            existing = json.load(f)\n",
    "        print(f\"No new results in repo folder\")\n",
    "        print(f\"âœ“ Existing backup preserved: {len(existing)} results\")\n",
    "    else:\n",
    "        print(\"No results to backup yet (first run)\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Safe to clone/pull now - results are backed up!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Clone Project from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "WORKDIR=\"/kaggle/working\"\n",
    "PROJECT_DIR=\"$WORKDIR/subdomain-playground\"\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Cloning Project from GitHub\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "mkdir -p \"$WORKDIR\"\n",
    "cd \"$WORKDIR\"\n",
    "\n",
    "if [ -d \"$PROJECT_DIR\" ]; then\n",
    "    echo \"Removing existing copy...\"\n",
    "    rm -rf \"$PROJECT_DIR\"\n",
    "fi\n",
    "\n",
    "git clone --depth 1 https://github.com/sayihhamza/subdomain-playground.git subdomain-playground\n",
    "\n",
    "if [ ! -d \"$PROJECT_DIR\" ]; then\n",
    "    echo \"âœ— Clone failed!\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "cd \"$PROJECT_DIR\"\n",
    "\n",
    "# Create data/scans directory for results\n",
    "mkdir -p data/scans\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Project cloned successfully!\"\n",
    "echo \"\"\n",
    "echo \"Project structure:\"\n",
    "ls -lh | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Go 1.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tmp\n",
    "\n",
    "if command -v sudo >/dev/null 2>&1; then\n",
    "    SUDO=\"sudo\"\n",
    "else\n",
    "    SUDO=\"\"\n",
    "fi\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Installing Go 1.24.1\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "$SUDO rm -rf /usr/local/go 2>/dev/null || true\n",
    "\n",
    "echo \"Downloading Go 1.24.1...\"\n",
    "for i in {1..3}; do\n",
    "    wget -q https://go.dev/dl/go1.24.1.linux-amd64.tar.gz -O /tmp/go.tar.gz && break || sleep 5\n",
    "done\n",
    "\n",
    "if [ ! -f /tmp/go.tar.gz ]; then\n",
    "    echo \"âœ— Failed to download Go\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "$SUDO tar -C /usr/local -xzf /tmp/go.tar.gz\n",
    "rm -f /tmp/go.tar.gz\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Go 1.24.1 installed!\"\n",
    "/usr/local/go/bin/go version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Build Security Tools\n",
    "\n",
    "Builds 5 tools (5-8 minutes):\n",
    "- **subfinder**: Passive subdomain enumeration\n",
    "- **findomain**: Additional passive enumeration (optional)\n",
    "- **httpx**: HTTP validation\n",
    "- **dnsx**: DNS + CNAME validation\n",
    "- **subzy**: Takeover detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=/usr/local/go/bin:$PATH\n",
    "cd /kaggle/working/subdomain-playground\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Building Security Tools\"\n",
    "echo \"==========================================\"\n",
    "echo \"\"\n",
    "\n",
    "mkdir -p bin\n",
    "\n",
    "# Build Go tools\n",
    "build_tool() {\n",
    "    local name=$1\n",
    "    local repo=$2\n",
    "    local index=$3\n",
    "    local total=$4\n",
    "    local max_attempts=3\n",
    "    \n",
    "    echo \"[$index/$total] Building $name...\"\n",
    "    \n",
    "    for attempt in $(seq 1 $max_attempts); do\n",
    "        if [ $attempt -gt 1 ]; then\n",
    "            echo \"  Retry $attempt/$max_attempts...\"\n",
    "            sleep 2\n",
    "        fi\n",
    "        \n",
    "        if GOBIN=$(pwd)/bin /usr/local/go/bin/go install -v ${repo}@latest 2>&1; then\n",
    "            if [ -f \"bin/$name\" ]; then\n",
    "                echo \"  âœ“ $name built successfully\"\n",
    "                return 0\n",
    "            fi\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    echo \"  âœ— Failed to build $name\"\n",
    "    return 1\n",
    "}\n",
    "\n",
    "# Download precompiled binary\n",
    "download_binary() {\n",
    "    local name=$1\n",
    "    local url=$2\n",
    "    local index=$3\n",
    "    local total=$4\n",
    "    local max_attempts=3\n",
    "    \n",
    "    echo \"[$index/$total] Downloading $name...\"\n",
    "    \n",
    "    for attempt in $(seq 1 $max_attempts); do\n",
    "        if [ $attempt -gt 1 ]; then\n",
    "            echo \"  Retry $attempt/$max_attempts...\"\n",
    "            sleep 2\n",
    "        fi\n",
    "        \n",
    "        if curl -sL \"$url\" -o /tmp/${name}.zip 2>&1; then\n",
    "            if unzip -q /tmp/${name}.zip -d bin/ 2>&1; then\n",
    "                if [ -f \"bin/$name\" ]; then\n",
    "                    chmod +x \"bin/$name\"\n",
    "                    echo \"  âœ“ $name downloaded successfully\"\n",
    "                    rm -f /tmp/${name}.zip\n",
    "                    return 0\n",
    "                fi\n",
    "            fi\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    echo \"  âš ï¸  $name download failed (optional)\"\n",
    "    rm -f /tmp/${name}.zip\n",
    "    return 1\n",
    "}\n",
    "\n",
    "# Build required tools\n",
    "build_tool \"subfinder\" \"github.com/projectdiscovery/subfinder/v2/cmd/subfinder\" \"1\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "download_binary \"findomain\" \"https://github.com/Findomain/Findomain/releases/latest/download/findomain-linux-i386.zip\" \"2\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"httpx\" \"github.com/projectdiscovery/httpx/cmd/httpx\" \"3\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"dnsx\" \"github.com/projectdiscovery/dnsx/cmd/dnsx\" \"4\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"subzy\" \"github.com/PentestPad/subzy\" \"5\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Verification\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "REQUIRED=\"subfinder httpx dnsx subzy\"\n",
    "TOOLS_OK=true\n",
    "\n",
    "for tool in $REQUIRED; do\n",
    "    if [ -f \"bin/$tool\" ]; then\n",
    "        echo \"  âœ“ bin/$tool\"\n",
    "    else\n",
    "        echo \"  âœ— bin/$tool MISSING\"\n",
    "        TOOLS_OK=false\n",
    "    fi\n",
    "done\n",
    "\n",
    "[ -f \"bin/findomain\" ] && echo \"  âœ“ bin/findomain (bonus!)\" || echo \"  âš ï¸  bin/findomain (optional)\"\n",
    "\n",
    "if [ \"$TOOLS_OK\" = false ]; then\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸  Required tools failed - re-run this cell\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ All required tools built!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SUBFINDER_PATH'] = '/kaggle/working/subdomain-playground/bin/subfinder'\n",
    "os.environ['FINDOMAIN_PATH'] = '/kaggle/working/subdomain-playground/bin/findomain'\n",
    "os.environ['DNSX_PATH'] = '/kaggle/working/subdomain-playground/bin/dnsx'\n",
    "os.environ['HTTPX_PATH'] = '/kaggle/working/subdomain-playground/bin/httpx'\n",
    "os.environ['SUBZY_PATH'] = '/kaggle/working/subdomain-playground/bin/subzy'\n",
    "\n",
    "os.chdir('/kaggle/working/subdomain-playground')\n",
    "\n",
    "print(\"âœ“ Environment configured\")\n",
    "print(f\"âœ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load and Sort Dataset\n",
    "\n",
    "**Configuration:**\n",
    "- `SORT_BY`: Column to sort by (default: 'Est Monthly Page Views')\n",
    "\n",
    "**What this cell does:**\n",
    "1. Loads the full CSV dataset from Kaggle input\n",
    "2. Sorts by specified column (descending)\n",
    "3. Creates `df_base_filtered` for Cell 7 and Cell 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ========================================\n",
    "# CONFIGURATION\n",
    "# ========================================\n",
    "SORT_BY = 'Est Monthly Page Views'  # Column to sort by\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING AND SORTING DATASET\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "\n",
    "# Load CSV from Kaggle input\n",
    "csv_path = '/kaggle/input/all-leads-merged/results.csv'\n",
    "print(f\"Loading: {csv_path}\")\n",
    "df = pd.read_csv(csv_path, low_memory=False)\n",
    "print(f\"âœ“ Loaded {len(df):,} total rows\")\n",
    "print(\"\")\n",
    "\n",
    "# Sort by specified column (NO FILTERING)\n",
    "print(f\"Sorting by: {SORT_BY} (descending)\")\n",
    "if SORT_BY in df.columns:\n",
    "    # Convert to numeric if it's a numeric column\n",
    "    df['_sort_numeric'] = pd.to_numeric(\n",
    "        df[SORT_BY].astype(str).str.replace(r'[^\\d.]', '', regex=True),\n",
    "        errors='coerce'\n",
    "    ).fillna(0)\n",
    "    df_sorted = df.sort_values('_sort_numeric', ascending=False)\n",
    "    print(f\"âœ“ Sorted by {SORT_BY}\")\n",
    "else:\n",
    "    print(f\"âš ï¸  Column '{SORT_BY}' not found - using unsorted\")\n",
    "    df_sorted = df\n",
    "print(\"\")\n",
    "\n",
    "# Store base dataset for Cell 7 (preview) and Cell 8 (scan)\n",
    "df_base_filtered = df_sorted.copy()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"âœ“ Dataset loaded and sorted\")\n",
    "print(f\"Total available: {len(df_base_filtered):,} rows\")\n",
    "print(\"\")\n",
    "print(\"Next steps:\")\n",
    "print(\"  â€¢ Cell 7: Apply filters + preview range\")\n",
    "print(\"  â€¢ Cell 8: Apply filters + scan range\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 7: Preview Dataset (Optional)\n",
    "\n",
    "**Configure preview range:**\n",
    "- `PREVIEW_START_ROW` and `PREVIEW_END_ROW`\n",
    "\n",
    "**Filters applied (for preview only):**\n",
    "1. Is_Shopify == 'Yes'\n",
    "2. Exclude *.myshopify.com domains\n",
    "3. Exclude HTTP 200 and 429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ========================================\n",
    "# PREVIEW CONFIGURATION (Cell 7 only)\n",
    "# ========================================\n",
    "PREVIEW_START_ROW = 1       # First row to preview (1-indexed)\n",
    "PREVIEW_END_ROW = 100       # Last row to preview\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(f\"PREVIEW: Rows {PREVIEW_START_ROW} to {PREVIEW_END_ROW} from Sorted Dataset\")\n",
    "print(\"=\" * 120)\n",
    "print(\"\")\n",
    "print(f\"Source: df_base_filtered from Cell 6 (sorted by {SORT_BY})\")\n",
    "print(f\"Total available: {len(df_base_filtered):,} rows\")\n",
    "print(\"\")\n",
    "\n",
    "# Select preview range from base dataset (sorted, but unfiltered)\n",
    "df_preview = df_base_filtered.iloc[PREVIEW_START_ROW-1:PREVIEW_END_ROW].copy()\n",
    "print(f\"Selected range: {len(df_preview)} rows\")\n",
    "print(\"\")\n",
    "\n",
    "# Apply preview filters (for display only)\n",
    "print(\"Applying filters for preview...\")\n",
    "\n",
    "# Filter A: Shopify stores only\n",
    "is_shopify = df_preview['Is_Shopify'] == 'Yes'\n",
    "print(f\"  After Shopify filter: {is_shopify.sum()} rows\")\n",
    "\n",
    "# Filter B: Exclude raw 'myshopify.com' domains\n",
    "is_custom_domain = ~df_preview['Subdomain'].str.contains('myshopify.com', case=False, na=False)\n",
    "print(f\"  After myshopify.com exclusion: {(is_shopify & is_custom_domain).sum()} rows\")\n",
    "\n",
    "# Filter C: Exclude HTTP 200 and 429\n",
    "excluded_statuses = [200, 429, '200', '429', 200.0, 429.0]\n",
    "is_interesting_status = ~df_preview['HTTP_Status'].isin(excluded_statuses)\n",
    "print(f\"  After status exclusion: {(is_shopify & is_custom_domain & is_interesting_status).sum()} rows\")\n",
    "\n",
    "# Apply all filters\n",
    "df_preview_filtered = df_preview[is_shopify & is_custom_domain & is_interesting_status].copy()\n",
    "print(\"\")\n",
    "\n",
    "# Prepare display with original row numbers\n",
    "display_df = df_preview_filtered.copy()\n",
    "# Calculate actual row numbers from base dataset\n",
    "actual_rows = []\n",
    "for idx in display_df.index:\n",
    "    actual_row = df_base_filtered.index.get_loc(idx) + 1\n",
    "    actual_rows.append(actual_row)\n",
    "display_df.insert(0, 'Row', actual_rows)\n",
    "\n",
    "# Select columns to display\n",
    "display_columns = ['Row', 'Subdomain', 'HTTP_Status', SORT_BY, 'CNAME_Record']\n",
    "display_columns = [col for col in display_columns if col in display_df.columns]\n",
    "\n",
    "print(\"=\" * 120)\n",
    "print(f\"Displaying {len(display_df)} domains after filters:\")\n",
    "print(\"=\" * 120)\n",
    "print(\"\")\n",
    "\n",
    "# Configure pandas display\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "pd.set_option('display.width', 120)\n",
    "\n",
    "# Display\n",
    "print(display_df[display_columns].to_string(index=False))\n",
    "\n",
    "print(\"\")\n",
    "print(\"=\" * 120)\n",
    "print(\"ðŸ“‹ Preview Summary:\")\n",
    "print(f\"  â€¢ Preview range: Rows {PREVIEW_START_ROW} to {PREVIEW_END_ROW}\")\n",
    "print(f\"  â€¢ After filters: {len(display_df)} domains\")\n",
    "print(f\"  â€¢ Sorted by: {SORT_BY} (highest first)\")\n",
    "print(f\"  â€¢ Filters: Shopify + Custom domain + Exclude status 200/429\")\n",
    "print(\"\")\n",
    "print(\"ðŸ’¡ NOTE: Cell 8 scans ALL domains in range (scan.py applies its own filters)\")\n",
    "print(\"=\" * 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 8: Deep Scan with Auto-Resume\n",
    "\n",
    "**How it works:**\n",
    "1. Progress saved to `/kaggle/working/scan_progress.json` every 50 domains\n",
    "2. Results saved to `/kaggle/working/all_results.json` after each session\n",
    "3. On re-run, automatically resumes from last completed row\n",
    "\n",
    "**Configuration:**\n",
    "- `SCAN_END_ROW` - Last row to scan (default: all rows)\n",
    "- `FORCE_RESTART = True` - Start from row 1, ignoring saved progress\n",
    "\n",
    "**Manual Resume (if needed):**\n",
    "```python\n",
    "import json\n",
    "with open('/kaggle/working/scan_progress.json', 'w') as f:\n",
    "    json.dump({'last_row': 2880, 'total_scanned': 2880}, f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ========================================\n",
    "# SCAN CONFIGURATION\n",
    "# ========================================\n",
    "SCAN_END_ROW = 1786654       # Last row to scan\n",
    "FORCE_RESTART = False        # Set to True to start from row 1\n",
    "\n",
    "# ========================================\n",
    "# FILE PATHS (persist in /kaggle/working)\n",
    "# ========================================\n",
    "PROGRESS_FILE = '/kaggle/working/scan_progress.json'\n",
    "BACKUP_RESULTS = '/kaggle/working/all_results.json'\n",
    "REPO_RESULTS = '/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json'\n",
    "\n",
    "def load_progress():\n",
    "    \"\"\"Load last scanned row from progress file\"\"\"\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        try:\n",
    "            with open(PROGRESS_FILE, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except:\n",
    "            pass\n",
    "    return {'last_row': 0, 'total_scanned': 0}\n",
    "\n",
    "def save_progress(last_row, total_scanned):\n",
    "    \"\"\"Save progress to file\"\"\"\n",
    "    with open(PROGRESS_FILE, 'w') as f:\n",
    "        json.dump({\n",
    "            'last_row': last_row,\n",
    "            'total_scanned': total_scanned,\n",
    "            'updated': datetime.now().isoformat()\n",
    "        }, f)\n",
    "\n",
    "def backup_results():\n",
    "    \"\"\"Merge repo results into backup file\"\"\"\n",
    "    if not os.path.exists(REPO_RESULTS):\n",
    "        return\n",
    "    \n",
    "    # Load current results\n",
    "    with open(REPO_RESULTS, 'r') as f:\n",
    "        current = json.load(f)\n",
    "    \n",
    "    if not current:\n",
    "        return\n",
    "    \n",
    "    # Load existing backup\n",
    "    existing = []\n",
    "    if os.path.exists(BACKUP_RESULTS):\n",
    "        with open(BACKUP_RESULTS, 'r') as f:\n",
    "            existing = json.load(f)\n",
    "    \n",
    "    # Merge (avoid duplicates)\n",
    "    existing_subs = {r['subdomain'] for r in existing}\n",
    "    new = [r for r in current if r['subdomain'] not in existing_subs]\n",
    "    merged = existing + new\n",
    "    \n",
    "    # Save\n",
    "    with open(BACKUP_RESULTS, 'w') as f:\n",
    "        json.dump(merged, f, indent=2)\n",
    "    \n",
    "    return len(merged), len(new)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SCAN WITH AUTO-RESUME\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\")\n",
    "\n",
    "# Determine starting row\n",
    "if FORCE_RESTART:\n",
    "    print(\"ðŸ”„ FORCE_RESTART = True â†’ Starting from row 1\")\n",
    "    if os.path.exists(PROGRESS_FILE):\n",
    "        os.remove(PROGRESS_FILE)\n",
    "    start_row = 1\n",
    "    total_scanned = 0\n",
    "else:\n",
    "    progress = load_progress()\n",
    "    start_row = progress['last_row'] + 1\n",
    "    total_scanned = progress['total_scanned']\n",
    "    if start_row > 1:\n",
    "        print(f\"ðŸ“‚ Resuming from saved progress!\")\n",
    "        print(f\"   Previously scanned: {total_scanned:,} domains\")\n",
    "        print(f\"   Last completed row: {progress['last_row']:,}\")\n",
    "        print(f\"   â†’ Starting from row: {start_row:,}\")\n",
    "    else:\n",
    "        print(\"ðŸ“ No previous progress â†’ Starting from row 1\")\n",
    "print(\"\")\n",
    "\n",
    "# Show backup status\n",
    "if os.path.exists(BACKUP_RESULTS):\n",
    "    with open(BACKUP_RESULTS, 'r') as f:\n",
    "        backup_count = len(json.load(f))\n",
    "    print(f\"ðŸ“¦ Backup contains: {backup_count} results\")\n",
    "    print(\"\")\n",
    "\n",
    "# Check if already complete\n",
    "if start_row > SCAN_END_ROW:\n",
    "    print(\"âœ… SCAN COMPLETE!\")\n",
    "    print(f\"   All {SCAN_END_ROW:,} rows have been processed.\")\n",
    "    print(f\"   Total scanned: {total_scanned:,}\")\n",
    "    print(\"\")\n",
    "    print(\"To restart, set FORCE_RESTART = True\")\n",
    "else:\n",
    "    print(f\"Source: df_base_filtered (sorted by {SORT_BY})\")\n",
    "    print(f\"Total in dataset: {len(df_base_filtered):,} rows\")\n",
    "    print(f\"Scan range: Row {start_row:,} to {SCAN_END_ROW:,}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Select remaining rows\n",
    "    df_scan = df_base_filtered.iloc[start_row-1:SCAN_END_ROW].copy()\n",
    "    domains_this_session = len(df_scan)\n",
    "    print(f\"Domains this session: {domains_this_session:,}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Create data directory and save targets\n",
    "    data_dir = Path('/kaggle/working/subdomain-playground/data')\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    scans_dir = data_dir / 'scans'\n",
    "    scans_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    targets_file = data_dir / 'all_sources.txt'\n",
    "    df_scan['Subdomain'].to_csv(targets_file, index=False, header=False)\n",
    "    print(f\"âœ“ Saved to: {targets_file}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Setup environment\n",
    "    os.chdir('/kaggle/working/subdomain-playground')\n",
    "    os.environ['PATH'] = f\"/kaggle/working/subdomain-playground/bin:{os.environ['PATH']}\"\n",
    "    os.environ['SUBFINDER_PATH'] = '/kaggle/working/subdomain-playground/bin/subfinder'\n",
    "    os.environ['FINDOMAIN_PATH'] = '/kaggle/working/subdomain-playground/bin/findomain'\n",
    "    os.environ['DNSX_PATH'] = '/kaggle/working/subdomain-playground/bin/dnsx'\n",
    "    os.environ['HTTPX_PATH'] = '/kaggle/working/subdomain-playground/bin/httpx'\n",
    "    os.environ['SUBZY_PATH'] = '/kaggle/working/subdomain-playground/bin/subzy'\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"STARTING SCAN\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\")\n",
    "\n",
    "    # Run scan\n",
    "    process = subprocess.Popen(\n",
    "        [sys.executable, '-u', 'scan.py',\n",
    "         '-l', 'data/all_sources.txt',\n",
    "         '--mode', 'full',\n",
    "         '--require-cname-contains', 'shopify',\n",
    "         '--filter-status', '3*,403,404,409,5*',\n",
    "         '--workers', '2'],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        universal_newlines=True,\n",
    "        bufsize=1\n",
    "    )\n",
    "\n",
    "    # Track progress from output\n",
    "    progress_pattern = re.compile(r'\\[(\\d+)/(\\d+)\\]')\n",
    "    domains_completed = 0\n",
    "    \n",
    "    for line in process.stdout:\n",
    "        print(line, end='', flush=True)\n",
    "        \n",
    "        # Parse [X/Y] progress\n",
    "        match = progress_pattern.search(line)\n",
    "        if match:\n",
    "            domains_completed = int(match.group(1))\n",
    "            \n",
    "            # Save progress every 50 domains\n",
    "            if domains_completed % 50 == 0:\n",
    "                current_row = start_row + domains_completed - 1\n",
    "                save_progress(current_row, total_scanned + domains_completed)\n",
    "                # Also backup results periodically\n",
    "                if domains_completed % 200 == 0:\n",
    "                    backup_results()\n",
    "\n",
    "    process.wait()\n",
    "    \n",
    "    # Final save\n",
    "    final_row = start_row + domains_completed - 1 if domains_completed > 0 else start_row - 1\n",
    "    final_total = total_scanned + domains_completed\n",
    "    save_progress(final_row, final_total)\n",
    "    \n",
    "    # Backup results\n",
    "    backup_info = backup_results()\n",
    "    \n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Session complete!\")\n",
    "    print(f\"   Domains this session: {domains_completed:,}\")\n",
    "    print(f\"   Total scanned so far: {final_total:,}\")\n",
    "    print(f\"   Last row completed: {final_row:,}\")\n",
    "    print(f\"   Remaining: {SCAN_END_ROW - final_row:,} rows\")\n",
    "    if backup_info:\n",
    "        print(f\"   Results backed up: {backup_info[0]:,} total ({backup_info[1]} new)\")\n",
    "    print(\"\")\n",
    "    print(\"Progress and results saved. Re-run this cell to continue.\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 9: View Results\n",
    "\n",
    "Views results from the backup file (`/kaggle/working/all_results.json`) which persists across sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Try backup file first, then repo file\n",
    "BACKUP_RESULTS = '/kaggle/working/all_results.json'\n",
    "REPO_RESULTS = '/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json'\n",
    "\n",
    "results = []\n",
    "results_file = None\n",
    "\n",
    "if os.path.exists(BACKUP_RESULTS):\n",
    "    results_file = BACKUP_RESULTS\n",
    "    with open(BACKUP_RESULTS, 'r') as f:\n",
    "        results = json.load(f)\n",
    "elif os.path.exists(REPO_RESULTS):\n",
    "    results_file = REPO_RESULTS\n",
    "    with open(REPO_RESULTS, 'r') as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "if results:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SCAN RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\")\n",
    "    print(f\"Source: {results_file}\")\n",
    "    print(f\"Total candidates found: {len(results)}\")\n",
    "\n",
    "    # Count by risk level\n",
    "    risk_counts = {}\n",
    "    for r in results:\n",
    "        risk = r.get(\"risk_level\", \"unknown\")\n",
    "        risk_counts[risk] = risk_counts.get(risk, 0) + 1\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Breakdown by risk:\")\n",
    "    for risk, count in sorted(risk_counts.items()):\n",
    "        print(f\"  {risk.upper()}: {count}\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TOP 20 FINDINGS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x.get(\"confidence_score\", 0), reverse=True)\n",
    "\n",
    "    for i, r in enumerate(sorted_results[:20], 1):\n",
    "        print(\"\")\n",
    "        print(f\"{i}. {r['subdomain']}\")\n",
    "        print(f\"   CNAME: {r.get('cname', 'N/A')}\")\n",
    "        print(f\"   HTTP Status: {r.get('http_status', 'N/A')}\")\n",
    "        print(f\"   Risk: {r.get('risk_level', 'N/A')}\")\n",
    "        print(f\"   Confidence: {r.get('confidence_score', 0)}\")\n",
    "else:\n",
    "    print(\"No results found yet.\")\n",
    "    print(\"Run Cell 9 (scan) first.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
