{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shopify Subdomain Takeover Scanner\n",
    "\n",
    "**Dataset-based workflow:**\n",
    "1. Load CSV dataset (`/kaggle/input/all-leads-merged/results.csv`)\n",
    "2. Filter: Shopify stores + exclude myshopify.com + exclude 200/429\n",
    "3. Sort by Est Monthly Page Views (highest traffic first)\n",
    "4. Select range to scan (e.g., rows 1-100)\n",
    "5. Deep scan with full mode\n",
    "\n",
    "**Total time:** ~7-9 hours for 10,000 domains (within Kaggle's 12-hour limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 1: Clone Project from GitHub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "WORKDIR=\"/kaggle/working\"\n",
    "PROJECT_DIR=\"$WORKDIR/subdomain-playground\"\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Cloning Project from GitHub\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "mkdir -p \"$WORKDIR\"\n",
    "cd \"$WORKDIR\"\n",
    "\n",
    "if [ -d \"$PROJECT_DIR\" ]; then\n",
    "    echo \"Removing existing copy...\"\n",
    "    rm -rf \"$PROJECT_DIR\"\n",
    "fi\n",
    "\n",
    "git clone --depth 1 https://github.com/sayihhamza/subdomain-playground.git subdomain-playground\n",
    "\n",
    "if [ ! -d \"$PROJECT_DIR\" ]; then\n",
    "    echo \"âœ— Clone failed!\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "cd \"$PROJECT_DIR\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Project cloned successfully!\"\n",
    "echo \"\"\n",
    "echo \"Project structure:\"\n",
    "ls -lh | head -20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 2: Install Python Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /kaggle/working/subdomain-playground\n",
    "\n",
    "echo \"Installing Python requirements...\"\n",
    "python3 -m pip install --quiet -r requirements.txt\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Dependencies installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 3: Install Go 1.24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /tmp\n",
    "\n",
    "if command -v sudo >/dev/null 2>&1; then\n",
    "    SUDO=\"sudo\"\n",
    "else\n",
    "    SUDO=\"\"\n",
    "fi\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Installing Go 1.24.1\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "$SUDO rm -rf /usr/local/go 2>/dev/null || true\n",
    "\n",
    "echo \"Downloading Go 1.24.1...\"\n",
    "for i in {1..3}; do\n",
    "    wget -q https://go.dev/dl/go1.24.1.linux-amd64.tar.gz -O /tmp/go.tar.gz && break || sleep 5\n",
    "done\n",
    "\n",
    "if [ ! -f /tmp/go.tar.gz ]; then\n",
    "    echo \"âœ— Failed to download Go\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "$SUDO tar -C /usr/local -xzf /tmp/go.tar.gz\n",
    "rm -f /tmp/go.tar.gz\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ Go 1.24.1 installed!\"\n",
    "/usr/local/go/bin/go version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 4: Build Security Tools\n",
    "\n",
    "Builds 5 tools (5-8 minutes):\n",
    "- **subfinder**: Passive subdomain enumeration\n",
    "- **findomain**: Additional passive enumeration (optional)\n",
    "- **httpx**: HTTP validation\n",
    "- **dnsx**: DNS + CNAME validation\n",
    "- **subzy**: Takeover detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "export PATH=/usr/local/go/bin:$PATH\n",
    "cd /kaggle/working/subdomain-playground\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Building Security Tools\"\n",
    "echo \"==========================================\"\n",
    "echo \"\"\n",
    "\n",
    "mkdir -p bin\n",
    "\n",
    "# Build Go tools\n",
    "build_tool() {\n",
    "    local name=$1\n",
    "    local repo=$2\n",
    "    local index=$3\n",
    "    local total=$4\n",
    "    local max_attempts=3\n",
    "    \n",
    "    echo \"[$index/$total] Building $name...\"\n",
    "    \n",
    "    for attempt in $(seq 1 $max_attempts); do\n",
    "        if [ $attempt -gt 1 ]; then\n",
    "            echo \"  Retry $attempt/$max_attempts...\"\n",
    "            sleep 2\n",
    "        fi\n",
    "        \n",
    "        if GOBIN=$(pwd)/bin /usr/local/go/bin/go install -v ${repo}@latest 2>&1; then\n",
    "            if [ -f \"bin/$name\" ]; then\n",
    "                echo \"  âœ“ $name built successfully\"\n",
    "                return 0\n",
    "            fi\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    echo \"  âœ— Failed to build $name\"\n",
    "    return 1\n",
    "}\n",
    "\n",
    "# Download precompiled binary\n",
    "download_binary() {\n",
    "    local name=$1\n",
    "    local url=$2\n",
    "    local index=$3\n",
    "    local total=$4\n",
    "    local max_attempts=3\n",
    "    \n",
    "    echo \"[$index/$total] Downloading $name...\"\n",
    "    \n",
    "    for attempt in $(seq 1 $max_attempts); do\n",
    "        if [ $attempt -gt 1 ]; then\n",
    "            echo \"  Retry $attempt/$max_attempts...\"\n",
    "            sleep 2\n",
    "        fi\n",
    "        \n",
    "        if curl -sL \"$url\" -o /tmp/${name}.zip 2>&1; then\n",
    "            if unzip -q /tmp/${name}.zip -d bin/ 2>&1; then\n",
    "                if [ -f \"bin/$name\" ]; then\n",
    "                    chmod +x \"bin/$name\"\n",
    "                    echo \"  âœ“ $name downloaded successfully\"\n",
    "                    rm -f /tmp/${name}.zip\n",
    "                    return 0\n",
    "                fi\n",
    "            fi\n",
    "        fi\n",
    "    done\n",
    "    \n",
    "    echo \"  âš ï¸  $name download failed (optional)\"\n",
    "    rm -f /tmp/${name}.zip\n",
    "    return 1\n",
    "}\n",
    "\n",
    "# Build required tools\n",
    "build_tool \"subfinder\" \"github.com/projectdiscovery/subfinder/v2/cmd/subfinder\" \"1\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "download_binary \"findomain\" \"https://github.com/Findomain/Findomain/releases/latest/download/findomain-linux-i386.zip\" \"2\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"httpx\" \"github.com/projectdiscovery/httpx/cmd/httpx\" \"3\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"dnsx\" \"github.com/projectdiscovery/dnsx/cmd/dnsx\" \"4\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "build_tool \"subzy\" \"github.com/PentestPad/subzy\" \"5\" \"5\"\n",
    "echo \"\"\n",
    "\n",
    "echo \"==========================================\"\n",
    "echo \"Verification\"\n",
    "echo \"==========================================\"\n",
    "\n",
    "REQUIRED=\"subfinder httpx dnsx subzy\"\n",
    "TOOLS_OK=true\n",
    "\n",
    "for tool in $REQUIRED; do\n",
    "    if [ -f \"bin/$tool\" ]; then\n",
    "        echo \"  âœ“ bin/$tool\"\n",
    "    else\n",
    "        echo \"  âœ— bin/$tool MISSING\"\n",
    "        TOOLS_OK=false\n",
    "    fi\n",
    "done\n",
    "\n",
    "[ -f \"bin/findomain\" ] && echo \"  âœ“ bin/findomain (bonus!)\" || echo \"  âš ï¸  bin/findomain (optional)\"\n",
    "\n",
    "if [ \"$TOOLS_OK\" = false ]; then\n",
    "    echo \"\"\n",
    "    echo \"âš ï¸  Required tools failed - re-run this cell\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ“ All required tools built!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 5: Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['SUBFINDER_PATH'] = '/kaggle/working/subdomain-playground/bin/subfinder'\n",
    "os.environ['FINDOMAIN_PATH'] = '/kaggle/working/subdomain-playground/bin/findomain'\n",
    "os.environ['DNSX_PATH'] = '/kaggle/working/subdomain-playground/bin/dnsx'\n",
    "os.environ['HTTPX_PATH'] = '/kaggle/working/subdomain-playground/bin/httpx'\n",
    "os.environ['SUBZY_PATH'] = '/kaggle/working/subdomain-playground/bin/subzy'\n",
    "\n",
    "os.chdir('/kaggle/working/subdomain-playground')\n",
    "\n",
    "print(\"âœ“ Environment configured\")\n",
    "print(f\"âœ“ Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell 6: Load and Filter Dataset\n",
    "\n",
    "**Configuration:**\n",
    "- `START_ROW`: First row to scan (default: 1)\n",
    "- `END_ROW`: Last row to scan (default: 100)\n",
    "- `SORT_BY`: Column to sort by (default: 'Est Monthly Page Views')\n",
    "\n",
    "**Filters applied:**\n",
    "1. Is_Shopify == 'Yes'\n",
    "2. Exclude *.myshopify.com\n",
    "3. Exclude HTTP 200 and 429\n",
    "4. Sort by specified column (descending)\n",
    "5. Select rows START_ROW to END_ROW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\n\n# ========================================\n# CONFIGURATION\n# ========================================\nSTART_ROW = 1      # First row to scan (1-indexed)\nEND_ROW = 100      # Last row to scan\nSORT_BY = 'Est Monthly Page Views'  # Column to sort by\n\nprint(\"=\" * 80)\nprint(\"LOADING AND FILTERING DATASET\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Load CSV\ncsv_path = '/kaggle/input/all-leads-merged/results.csv'\nprint(f\"Loading: {csv_path}\")\ndf = pd.read_csv(csv_path, low_memory=False)\nprint(f\"âœ“ Loaded {len(df):,} total rows\")\nprint(\"\")\n\n# Filter A: Shopify stores only\nis_shopify = df['Is_Shopify'] == 'Yes'\n\n# Filter B: Exclude raw 'myshopify.com' domains\nis_custom_domain = ~df['Subdomain'].str.contains('myshopify.com', case=False, na=False)\n\n# Filter C: Exclude HTTP 200 and 429\nexcluded_statuses = [200, 429, '200', '429', 200.0, 429.0]\nis_interesting_status = ~df['HTTP_Status'].isin(excluded_statuses)\n\n# Apply all filters\ndf_filtered = df[is_shopify & is_custom_domain & is_interesting_status].copy()\n\nprint(\"Filters applied:\")\nprint(f\"  Original rows: {len(df):,}\")\nprint(f\"  After Shopify filter: {is_shopify.sum():,}\")\nprint(f\"  After myshopify.com exclusion: {(is_shopify & is_custom_domain).sum():,}\")\nprint(f\"  After status exclusion: {len(df_filtered):,}\")\nprint(\"\")\n\n# Status breakdown\nprint(\"Status code breakdown (after filters):\")\nstatus_counts = df_filtered['HTTP_Status'].value_counts().head(10)\nfor status, count in status_counts.items():\n    print(f\"  {status}: {count:,}\")\nprint(\"\")\n\n# Sort by specified column\nprint(f\"Sorting by: {SORT_BY} (descending)\")\nif SORT_BY in df_filtered.columns:\n    # Convert to numeric if it's a numeric column\n    df_filtered['_sort_numeric'] = pd.to_numeric(\n        df_filtered[SORT_BY].astype(str).str.replace(r'[^\\d.]', '', regex=True),\n        errors='coerce'\n    ).fillna(0)\n    df_sorted = df_filtered.sort_values('_sort_numeric', ascending=False)\n    print(f\"âœ“ Sorted by {SORT_BY}\")\nelse:\n    print(f\"âš ï¸  Column '{SORT_BY}' not found - using unsorted\")\n    df_sorted = df_filtered\nprint(\"\")\n\n# Select range\nprint(f\"Selecting rows {START_ROW} to {END_ROW}\")\ndf_selected = df_sorted.iloc[START_ROW-1:END_ROW].copy()\nprint(f\"âœ“ Selected {len(df_selected)} domains\")\nprint(\"\")\n\n# Store for next cells (Cell 7 and Cell 8 will use their own ranges)\n# This creates the base filtered/sorted dataset\ndf_base_filtered = df_sorted.copy()\n\nprint(\"=\" * 80)\nprint(\"Dataset loaded and filtered\")\nprint(f\"Total available after filtering: {len(df_base_filtered):,} rows\")\nprint(\"\")\nprint(\"Next: Configure range in Cell 7 (preview) and Cell 8 (scan)\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "source": "## Cell 7: Preview Full Dataset (Manual Review)\n\n**This cell displays the complete filtered and sorted dataset before scanning.**\n\nYou can manually check subdomains while the scan runs in Cell 8.\n\n**Columns shown:**\n- Row (1-based index for easy reference)\n- Subdomain (verified to be subdomain, not root domain)\n- HTTP Status\n- Est Monthly Page Views (sorted descending)\n- CNAME Record\n\n**Use this to:**\n- Verify the filtered data looks correct\n- Check specific subdomains manually while scan runs\n- Confirm the range (START_ROW to END_ROW)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\n\n# ========================================\n# PREVIEW CONFIGURATION (Cell 7 only)\n# ========================================\nPREVIEW_START_ROW = 1       # First row to preview (1-indexed)\nPREVIEW_END_ROW = 100       # Last row to preview\n\nprint(\"=\" * 120)\nprint(f\"PREVIEW: Filtered Dataset (Rows {PREVIEW_START_ROW} to {PREVIEW_END_ROW})\")\nprint(\"=\" * 120)\nprint(\"\")\nprint(f\"Total available after filtering: {len(df_base_filtered):,} rows\")\nprint(f\"Sorted by: {SORT_BY} (descending)\")\nprint(\"\")\n\n# Select preview range from base filtered dataset\ndf_preview = df_base_filtered.iloc[PREVIEW_START_ROW-1:PREVIEW_END_ROW].copy()\n\n# Verify all entries are subdomains (not root domains)\ndef is_subdomain(domain):\n    \"\"\"Check if domain is a subdomain (has more than 2 parts)\"\"\"\n    if pd.isna(domain) or not isinstance(domain, str):\n        return False\n    parts = domain.split('.')\n    return len(parts) > 2\n\n# Filter to only show subdomains\nsubdomain_mask = df_preview['Subdomain'].apply(is_subdomain)\ndf_preview_subdomains = df_preview[subdomain_mask].copy()\nnon_subdomain_count = len(df_preview) - len(df_preview_subdomains)\n\nif non_subdomain_count > 0:\n    print(f\"âš ï¸  Filtered out {non_subdomain_count} root domains (kept only subdomains)\")\n    print(\"\")\n\n# Prepare display with row numbers\ndisplay_df = df_preview_subdomains.copy()\ndisplay_df.insert(0, 'Row', range(PREVIEW_START_ROW, PREVIEW_START_ROW + len(display_df)))\n\n# Select columns to display\ndisplay_columns = ['Row', 'Subdomain', 'HTTP_Status', SORT_BY, 'CNAME_Record']\ndisplay_columns = [col for col in display_columns if col in display_df.columns]\n\nprint(\"=\" * 120)\nprint(f\"Displaying {len(display_df)} subdomains:\")\nprint(\"=\" * 120)\nprint(\"\")\n\n# Configure pandas display options\npd.set_option('display.max_rows', None)  # Show ALL rows\npd.set_option('display.max_colwidth', 50)\npd.set_option('display.width', 120)\n\n# Display the full preview\nprint(display_df[display_columns].to_string(index=False))\n\nprint(\"\")\nprint(\"=\" * 120)\nprint(\"\")\nprint(\"ðŸ“‹ Preview Summary:\")\nprint(f\"  â€¢ Total subdomains shown: {len(display_df)}\")\nprint(f\"  â€¢ Preview range: Row {PREVIEW_START_ROW} to {PREVIEW_START_ROW + len(display_df) - 1}\")\nprint(f\"  â€¢ Sorted by: {SORT_BY} (highest first)\")\nprint(f\"  â€¢ All entries verified as subdomains (not root domains)\")\nprint(\"\")\nprint(\"ðŸ’¡ TIP: You can manually review these subdomains while Cell 8 runs the scan\")\nprint(\"\")\nprint(\"=\" * 120)\nprint(\"Next: Configure scan range in Cell 8\")\nprint(\"=\" * 120)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 8: Deep Scan Execution\n\n**Scan configuration:**\n- Mode: `full` (active + passive enumeration)\n- Provider: Shopify\n- Filters: CNAME contains 'shopify' + HTTP status 3xx/4xx/5xx\n- Workers: 2 (optimized for Kaggle)\n\n**Note:** Since inputs are already subdomains, the scanner will:\n- Skip subdomain enumeration (saves hours!)\n- Go directly to DNS/HTTP validation\n- Check for takeover vulnerabilities\n\n**Time:** While this runs, you can review the subdomains from Cell 7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import subprocess\nimport sys\nimport pandas as pd\n\n# ========================================\n# SCAN CONFIGURATION (Cell 8 only)\n# ========================================\nSCAN_START_ROW = 1       # First row to scan (1-indexed)\nSCAN_END_ROW = 100       # Last row to scan\n\nprint(\"=\" * 80)\nprint(\"PREPARING SCAN\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Select scan range from base filtered dataset\ndf_scan = df_base_filtered.iloc[SCAN_START_ROW-1:SCAN_END_ROW].copy()\n\n# Verify all entries are subdomains (not root domains)\ndef is_subdomain(domain):\n    \"\"\"Check if domain is a subdomain (has more than 2 parts)\"\"\"\n    if pd.isna(domain) or not isinstance(domain, str):\n        return False\n    parts = domain.split('.')\n    return len(parts) > 2\n\n# Filter to only scan subdomains\nsubdomain_mask = df_scan['Subdomain'].apply(is_subdomain)\ndf_scan_subdomains = df_scan[subdomain_mask].copy()\nnon_subdomain_count = len(df_scan) - len(df_scan_subdomains)\n\nprint(f\"Scan Range: Rows {SCAN_START_ROW} to {SCAN_END_ROW}\")\nprint(f\"Total in range: {len(df_scan)}\")\nif non_subdomain_count > 0:\n    print(f\"Filtered out: {non_subdomain_count} root domains\")\nprint(f\"Subdomains to scan: {len(df_scan_subdomains)}\")\nprint(\"\")\n\n# Save to file for scanning\ntargets_file = '/kaggle/working/subdomain-playground/data/all_sources.txt'\ndf_scan_subdomains['Subdomain'].to_csv(targets_file, index=False, header=False)\nprint(f\"âœ“ Saved {len(df_scan_subdomains)} targets to: {targets_file}\")\nprint(\"\")\n\nprint(\"=\" * 80)\nprint(\"STARTING FULL SCAN\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Count domains\nwith open('data/all_sources.txt', 'r') as f:\n    domain_count = len(f.readlines())\n\nprint(f\"Targets: {domain_count} domains\")\nprint(\"Mode: full (active + passive enumeration)\")\nprint(\"Workers: 2\")\nprint(\"\")\nprint(\"Filters:\")\nprint(\"  âœ“ CNAME contains 'shopify'\")\nprint(\"  âœ“ HTTP status 3xx, 4xx, 5xx\")\nprint(\"\")\nprint(\"Features:\")\nprint(\"  âœ“ Automatic subdomain detection (skips enumeration)\")\nprint(\"  âœ“ CNAME blacklist (46 patterns)\")\nprint(\"  âœ“ Cloudflare verification detection\")\nprint(\"  âœ“ Tools: subfinder + findomain + dnsx + httpx + subzy\")\nprint(\"\")\nprint(\"Estimated time: 7-9 hours for 10k domains\")\nprint(\"\")\nprint(\"=\" * 80)\nprint(\"\")\n\n# Run scan\nprocess = subprocess.Popen(\n    [sys.executable, '-u', 'scan.py',\n     '-l', 'data/all_sources.txt',\n     '--mode', 'full',\n     '--require-cname-contains', 'shopify',\n     '--filter-status', '3*,4*,5*',\n     '--workers', '2'],\n    stdout=subprocess.PIPE,\n    stderr=subprocess.STDOUT,\n    universal_newlines=True,\n    bufsize=1\n)\n\n# Stream output\nfor line in process.stdout:\n    print(line, end='', flush=True)\n\nprocess.wait()\nprint(\"\")\nprint(\"=\" * 80)\nprint(f\"Scan completed with return code: {process.returncode}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 9: View Results"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "results_file = Path(\"/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json\")\n",
    "\n",
    "if results_file.exists():\n",
    "    with results_file.open(\"r\") as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"SCAN RESULTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\")\n",
    "    print(f\"Total candidates found: {len(results)}\")\n",
    "\n",
    "    # Count by risk level\n",
    "    risk_counts = {}\n",
    "    for r in results:\n",
    "        risk = r.get(\"risk_level\", \"unknown\")\n",
    "        risk_counts[risk] = risk_counts.get(risk, 0) + 1\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Breakdown by risk:\")\n",
    "    for risk, count in sorted(risk_counts.items()):\n",
    "        print(f\"  {risk.upper()}: {count}\")\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"TOP 10 FINDINGS\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    sorted_results = sorted(results, key=lambda x: x.get(\"confidence_score\", 0), reverse=True)\n",
    "\n",
    "    for i, r in enumerate(sorted_results[:10], 1):\n",
    "        print(\"\")\n",
    "        print(f\"{i}. {r['subdomain']}\")\n",
    "        print(f\"   CNAME: {r.get('cname', 'N/A')}\")\n",
    "        print(f\"   HTTP Status: {r.get('http_status', 'N/A')}\")\n",
    "        print(f\"   Risk: {r.get('risk_level', 'N/A')}\")\n",
    "        print(f\"   Confidence: {r.get('confidence_score', 0)}\")\n",
    "else:\n",
    "    print(f\"âœ— Results file not found: {results_file}\")\n",
    "    print(\"Make sure the scan completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Cell 10: Export to CSV"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "results_file = Path(\"/kaggle/working/subdomain-playground/data/scans/shopify_takeover_candidates.json\")\n",
    "\n",
    "if not results_file.exists():\n",
    "    print(\"âœ— Results file not found. Run the scan first.\")\n",
    "else:\n",
    "    with results_file.open(\"r\") as f:\n",
    "        results = json.load(f)\n",
    "\n",
    "    if not results:\n",
    "        print(\"âœ— No results to export.\")\n",
    "    else:\n",
    "        df = pd.DataFrame(results)\n",
    "\n",
    "        # Select key columns\n",
    "        columns = [\n",
    "            \"subdomain\", \"cname\", \"http_status\", \"risk_level\", \"confidence_score\",\n",
    "            \"cname_chain_count\", \"final_cname_target\", \"a_records\", \"provider\"\n",
    "        ]\n",
    "        df_export = df[[col for col in columns if col in df.columns]]\n",
    "        df_export = df_export.sort_values(\"confidence_score\", ascending=False)\n",
    "\n",
    "        # Save to CSV\n",
    "        output_csv = Path(\"/kaggle/working/shopify_takeovers.csv\")\n",
    "        df_export.to_csv(output_csv, index=False)\n",
    "\n",
    "        print(\"=\" * 80)\n",
    "        print(\"CSV EXPORT\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"\")\n",
    "        print(f\"âœ“ Exported {len(df_export)} results to: {output_csv}\")\n",
    "        print(\"\")\n",
    "        print(\"Preview (top 20):\")\n",
    "        print(df_export.head(20).to_string(index=False))\n",
    "        \n",
    "        # High-risk only\n",
    "        df_high = df_export[df_export[\"risk_level\"].isin([\"critical\", \"high\"])]\n",
    "        if len(df_high) > 0:\n",
    "            high_risk_csv = Path(\"/kaggle/working/shopify_high_risk.csv\")\n",
    "            df_high.to_csv(high_risk_csv, index=False)\n",
    "            print(\"\")\n",
    "            print(f\"âœ“ High-risk only ({len(df_high)} results): {high_risk_csv}\")\n",
    "        \n",
    "        print(\"\")\n",
    "        print(\"=\" * 80)\n",
    "        print(\"âœ… COMPLETE!\")\n",
    "        print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}